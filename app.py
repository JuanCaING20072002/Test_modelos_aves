 

import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import os
from sklearn.metrics import confusion_matrix, classification_report

# --- Lazy imports for heavy ML libs (allow deploy without TF/JAX/PennyLane) ---
HAS_TF = True
HAS_PENNYLANE = True
# Placeholders for symbols that normally come from TF/Keras
tf = None
load_img = None
img_to_array = None
image_dataset_from_directory = None
vgg16_preprocess = None
try:
    import tensorflow as tf
    # Import Keras helpers from tensorflow.keras inside the try so missing TF
    # doesn't crash the app at import time on Streamlit Cloud when requirements
    # omit heavy ML packages.
    try:
        from tensorflow.keras.utils import load_img, img_to_array, image_dataset_from_directory
        from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess
    except Exception:
        # If keras helpers aren't available, keep placeholders as None
        load_img = None
        img_to_array = None
        image_dataset_from_directory = None
        vgg16_preprocess = None
    HAS_TF = True
except Exception:
    tf = None
    load_img = None
    img_to_array = None
    image_dataset_from_directory = None
    vgg16_preprocess = None
    HAS_TF = False

try:
    import pennylane as _qml
    HAS_PENNYLANE = True
except Exception:
    HAS_PENNYLANE = False

# NOTE: touch for deploy/no-tf - ensure Streamlit Cloud picks up latest commit
# deploy-touch: 2025-11-02 21:10:22 -0500

# ============================
# Configuraci√≥n inicial
# ============================
st.set_page_config(page_title="Clasificaci√≥n de Aves", page_icon="ü¶ú", layout="wide")
st.title("ü¶ú Clasificador de Aves ‚Äì Demo")
st.caption("Sube una imagen para identificar la especie o revisa el desempe√±o del modelo en el conjunto de validaci√≥n.")

# Directorio ra√≠z de modelos
MODELS_DIR = "modelos"
BATCH_SIZE = 32 # Tama√±o de lote para evaluaci√≥n
IMG_SIZE = (224, 224)  # Forma esperada por VGG16
# ============================
# Cargar modelo entrenado
# ============================
# Cargar modelo entrenado (SavedModel) usando Keras 3


@st.cache_resource(show_spinner=False)
def load_model_generic(path: str):
    """Cargar un modelo: puede ser SavedModel (dir con saved_model.pb) o .keras/.h5 Keras file.
    Devuelve un objeto invocable (TFSMLayer o tf.keras.Model).
    """
    # SavedModel directory -> TFSMLayer (inferencia v√≠a signature)
    if os.path.isdir(path) and os.path.isfile(os.path.join(path, "saved_model.pb")):
        if not HAS_TF:
            raise RuntimeError("TensorFlow no est√° disponible en este despliegue. Cargue el modelo localmente en `.venv310` para usar SavedModel.")
        # Importar TFSMLayer localmente para evitar error en entornos sin TF
        try:
            from keras.layers import TFSMLayer
        except Exception:
            try:
                # Alternativa: tensorflow.keras.layers (si aplica)
                from tensorflow.keras.layers import TFSMLayer
            except Exception:
                raise RuntimeError("TFSMLayer no est√° disponible en este entorno. No se puede cargar SavedModel.")
        return TFSMLayer(path, call_endpoint="serving_default")

    # Keras file (.keras, .h5) -> tf.keras.models.load_model
    if os.path.isfile(path) and path.lower().endswith((".keras", ".h5", ".hdf5")):
        # importar dentro de la funci√≥n para evitar error si pennylane no est√° instalado
        from tensorflow import keras
        # Intentar cargar el .keras manejando la capa KerasLayer de PennyLane
        # Si pennylane est√° instalado, pasamos la clase en `custom_objects` para
        # que Keras pueda deserializar la capa correctamente.
        # Intentar cargar el .keras manejando la capa KerasLayer de PennyLane.
        # Si pennylane est√° instalado, intentaremos primero una carga normal; si falla
        # por faltar el argumento `qnode`, construiremos un loader que recree un
        # qnode razonable y volvamos a intentar con `custom_objects`.
        if not HAS_TF:
            raise RuntimeError("TensorFlow no est√° disponible en este despliegue. Cargue el modelo localmente en `.venv310` para usar archivos .keras.")
        try:
            try:
                import pennylane as qml
                from pennylane.qnn.keras import KerasLayer as PLKerasLayer
            except Exception:
                qml = None
                PLKerasLayer = None

            # Intento de carga simple primero
            try:
                model = keras.models.load_model(path, compile=False)
                return model
            except Exception as e_load:
                # Si falla y parece relacionado con KerasLayer/qnode, intentar fallback
                msg = str(e_load)
                if qml is None or "KerasLayer" not in msg and "qnode" not in msg:
                    # No parece un error solucionable aqu√≠: propagar con mensaje informativo
                    raise RuntimeError(
                        "Error cargando el archivo .keras. Si el modelo contiene una capa de PennyLane (KerasLayer), "
                        "aseg√∫rate de tener instalado `pennylane` y versiones compatibles de TensorFlow/Keras. "
                        f"Error original: {e_load}"
                    ) from e_load

                # Construir una clase fallback con `from_config` y pasarla en custom_objects.
                # Esto evita registrar una clase serializable (que requiere get_config()).
                def build_keraslayer_fallback():
                    try:
                        import pennylane as _qml
                        from pennylane.qnn.keras import KerasLayer as _PLKerasLayer

                        class KerasLayerFallback:
                            @classmethod
                            def from_config(cls, config, custom_objects=None):
                                # Intentar inferir informaci√≥n √∫til desde la config
                                weight_shapes = config.get("weight_shapes", None)
                                output_dim = config.get("output_dim", None)
                                ws = None
                                try:
                                    # Algunos serializados ponen estructuras varias en weight_shapes
                                    if isinstance(weight_shapes, dict):
                                        ws = weight_shapes.get("weights") or weight_shapes
                                    else:
                                        ws = weight_shapes
                                except Exception:
                                    ws = weight_shapes

                                # heur√≠stica para n_qubits: preferir inferir de output_dim si es potencia de 2
                                import math

                                def is_power_of_two(n):
                                    return n > 0 and (n & (n - 1)) == 0

                                n_qubits = None
                                if output_dim is not None:
                                    try:
                                        od = int(output_dim)
                                        if is_power_of_two(od):
                                            n_qubits = int(math.log2(od))
                                    except Exception:
                                        n_qubits = None

                                # Si no deducido, intentar heur√≠stica desde weight_shapes
                                if n_qubits is None:
                                    n_qubits = 4
                                    if isinstance(ws, (list, tuple)) and len(ws) >= 2:
                                        try:
                                            # heur√≠stica antigua: ws[1] era n_qubits en algunos modelos
                                            candidate = int(ws[1])
                                            if 1 <= candidate <= 16:
                                                n_qubits = candidate
                                        except Exception:
                                            pass

                                dev = _qml.device("default.qubit", wires=n_qubits)

                                # Construir circuito en funci√≥n de output_dim:
                                # - Si output_dim es potencia de 2 y coincide con 2**n_qubits -> devolver probs
                                # - Si output_dim es peque√±o y no potencia de 2 -> devolver expectativas (PauliZ)
                                def circuit_probs(inputs, weights):
                                    # Evitar AngleEmbedding (validaciones estrictas de features).
                                    # Hacemos un embedding simple: aplicar RY con entradas a cada qubit
                                    L = int(getattr(inputs, "shape", (len(inputs),))[-1]) if hasattr(inputs, "shape") else len(inputs)
                                    for j in range(n_qubits):
                                        idx = j % L
                                        _qml.RY(inputs[idx], wires=j)
                                    _qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))
                                    return _qml.probs(wires=list(range(n_qubits)))

                                def circuit_expvals(inputs, weights, n_out):
                                    # Embedding manual: map elementos de inputs a qubits por rotaciones RY
                                    L = int(getattr(inputs, "shape", (len(inputs),))[-1]) if hasattr(inputs, "shape") else len(inputs)
                                    for j in range(n_qubits):
                                        idx = j % L
                                        _qml.RY(inputs[idx], wires=j)
                                    _qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))
                                    res = []
                                    for i in range(n_out):
                                        wire = i % n_qubits
                                        res.append(_qml.expval(_qml.PauliZ(wire)))
                                    return tuple(res) if len(res) > 1 else res[0]

                                # Elegir tipo de medici√≥n
                                use_probs = False
                                if output_dim is not None:
                                    try:
                                        od = int(output_dim)
                                        if is_power_of_two(od) and 2 ** n_qubits == od:
                                            use_probs = True
                                        else:
                                            use_probs = False
                                    except Exception:
                                        use_probs = False
                                else:
                                    # sin output_dim asumimos probs (com√∫n en ejemplos)
                                    use_probs = True

                                if use_probs:
                                    qnode = _qml.QNode(circuit_probs, dev, interface="tf")
                                else:
                                    # cuando usamos expvals debemos envolver para devolver vector
                                    def qfunc(inputs, weights):
                                        return circuit_expvals(inputs, weights, output_dim or n_qubits)

                                    qnode = _qml.QNode(qfunc, dev, interface="tf")

                                # Crear KerasLayer pasando weight_shapes y output_dim si es posible
                                try:
                                    if output_dim is not None:
                                        return _PLKerasLayer(qnode, weight_shapes=weight_shapes, output_dim=output_dim)
                                    else:
                                        return _PLKerasLayer(qnode, weight_shapes=weight_shapes)
                                except Exception:
                                    # √∫ltimo recurso: crear sin weight_shapes
                                    if output_dim is not None:
                                        return _PLKerasLayer(qnode, output_dim=output_dim)
                                    return _PLKerasLayer(qnode)

                        return KerasLayerFallback
                    except Exception:
                        return None

                fallback_cls = build_keraslayer_fallback()
                custom = {}
                if fallback_cls is not None:
                    custom["KerasLayer"] = fallback_cls
                    custom["pennylane.qnn.keras.KerasLayer"] = fallback_cls
                elif PLKerasLayer is not None:
                    custom["KerasLayer"] = PLKerasLayer
                    custom["pennylane.qnn.keras.KerasLayer"] = PLKerasLayer

                model = keras.models.load_model(path, compile=False, custom_objects=custom if custom else None)
                # Si el modelo contiene capas de PennyLane (KerasLayer) que usan embeddings
                # originales (AngleEmbedding), reemplazamos su qnode por una versi√≥n
                # que usa embedding manual (RY) para evitar errores de longitud de features.
                try:
                    import pennylane as _qml2
                    from pennylane.qnn.keras import KerasLayer as _PLKerasLayer2

                    def rebuild_qnode_for_layer(layer):
                        try:
                            cfg = layer.get_config() or {}
                        except Exception:
                            cfg = {}
                        weight_shapes = cfg.get("weight_shapes", None)
                        output_dim = cfg.get("output_dim", None)

                        # heur√≠stica n_qubits
                        import math

                        def is_power_of_two(n):
                            return n > 0 and (n & (n - 1)) == 0

                        n_qubits = None
                        if output_dim is not None:
                            try:
                                od = int(output_dim)
                                if is_power_of_two(od):
                                    n_qubits = int(math.log2(od))
                            except Exception:
                                n_qubits = None

                        if n_qubits is None:
                            n_qubits = 4
                            try:
                                ws = weight_shapes
                                if isinstance(ws, dict):
                                    ws = ws.get("weights") or ws
                                if isinstance(ws, (list, tuple)) and len(ws) >= 2:
                                    cand = int(ws[1])
                                    if 1 <= cand <= 32:
                                        n_qubits = cand
                            except Exception:
                                pass

                        dev = _qml2.device("default.qubit", wires=n_qubits)

                        def q_embed_ry(inputs, weights):
                            L = int(getattr(inputs, "shape", (len(inputs),))[-1]) if hasattr(inputs, "shape") else len(inputs)
                            for j in range(n_qubits):
                                idx = j % L
                                _qml2.RY(inputs[idx], wires=j)
                            _qml2.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))
                            # devolver probs si coincide con potencia de dos
                            if output_dim is not None:
                                try:
                                    od = int(output_dim)
                                    if is_power_of_two(od) and 2 ** n_qubits == od:
                                        return _qml2.probs(wires=list(range(n_qubits)))
                                except Exception:
                                    pass
                            # por defecto devolver expectativas PauliZ hasta output_dim
                            n_out = int(output_dim) if output_dim is not None else n_qubits
                            res = []
                            for i in range(n_out):
                                res.append(_qml2.expval(_qml2.PauliZ(i % n_qubits)))
                            return tuple(res) if len(res) > 1 else res[0]

                        qnode = _qml2.QNode(q_embed_ry, dev, interface="tf")
                        try:
                            # intentar reemplazar el qnode del layer
                            layer.qnode = qnode
                            try:
                                layer._qnode = qnode
                            except Exception:
                                pass
                            return True
                        except Exception:
                            return False

                    # Aplicar sobre todas las capas si el modelo es un tf.keras.Model
                    if hasattr(model, "layers"):
                        for lyr in model.layers:
                            try:
                                if isinstance(lyr, _PLKerasLayer2):
                                    rebuild_qnode_for_layer(lyr)
                            except Exception:
                                # ignorar capas que no se puedan procesar
                                pass
                except Exception:
                    # Si pennylane no est√° disponible o ocurre error, continuar sin parche
                    pass

                return model
        except Exception as e:
            # Mejor mensaje de error para el usuario: indicar que puede necesitar pennylane
            raise RuntimeError(
                "Error cargando el archivo .keras. Si el modelo contiene una capa de PennyLane (KerasLayer), "
                "aseg√∫rate de tener instalado `pennylane` y una versi√≥n de TensorFlow/Keras compatible. "
                f"Error original: {e}"
            )

    raise ValueError(f"Ruta de modelo no v√°lida o formato no soportado: {path}")

# ============================
# Datasets y utilidades
# ============================

def get_dataset(split: str = "valid", img_size=IMG_SIZE):
    directory = os.path.join("datos", split)
    if image_dataset_from_directory is None:
        raise RuntimeError("Funci√≥n image_dataset_from_directory no disponible: TensorFlow no est√° instalado en este despliegue.")
    ds = image_dataset_from_directory(
        directory,
        image_size=img_size,
        batch_size=BATCH_SIZE,
        shuffle=False,
    )
    return ds


def get_model_diagnostics(model):
    """Recolectar informaci√≥n √∫til sobre el modelo y sus capas para depuraci√≥n.
    Devuelve una lista de dicts con: name, class, input_shape, output_shape, extra (config/qnode info).
    """
    diagnostics = []
    try:
        from tensorflow.keras.models import Model
    except Exception:
        Model = None

    def _shape_to_list(s):
        try:
            if hasattr(s, "as_list"):
                return s.as_list()
            return tuple(int(x) if x is not None else None for x in s)
        except Exception:
            return str(s)

    if hasattr(model, "layers"):
        for lyr in getattr(model, "layers", []):
            info = {
                "name": getattr(lyr, "name", str(type(lyr))),
                "class": type(lyr).__name__,
                "input_shape": None,
                "output_shape": None,
                "extra": {},
            }
            try:
                if hasattr(lyr, "input_shape"):
                    info["input_shape"] = _shape_to_list(lyr.input_shape)
                elif hasattr(lyr, "input_spec") and getattr(lyr, "input_spec") is not None:
                    spec = lyr.input_spec
                    if isinstance(spec, (list, tuple)):
                        spec = spec[0]
                    if hasattr(spec, "shape"):
                        info["input_shape"] = _shape_to_list(spec.shape)
            except Exception:
                pass
            try:
                if hasattr(lyr, "output_shape"):
                    info["output_shape"] = _shape_to_list(lyr.output_shape)
            except Exception:
                pass

            # Si es una KerasLayer de PennyLane, a√±adir config y qnode_weights
            try:
                modname = type(lyr).__module__
                clsname = type(lyr).__name__
                if "pennylane" in modname or clsname.lower().find("keraslayer") >= 0:
                    try:
                        cfg = lyr.get_config()
                        info["extra"]["config"] = cfg
                    except Exception:
                        info["extra"]["config"] = "<no get_config()>"
                    try:
                        qw = getattr(lyr, "qnode_weights", None)
                        if isinstance(qw, dict):
                            info["extra"]["qnode_weights"] = {k: getattr(v, "shape", str(type(v))) for k, v in qw.items()}
                        else:
                            info["extra"]["qnode_weights"] = str(type(qw))
                    except Exception:
                        info["extra"]["qnode_weights"] = "<error>"
            except Exception:
                pass

            diagnostics.append(info)
    else:
        diagnostics.append({"model": str(type(model)), "note": "No layers attribute"})

    return diagnostics

# Funci√≥n para cargar nombres de clases
def list_available_models():
    models = {}
    if os.path.isdir(MODELS_DIR):
        for name in sorted(os.listdir(MODELS_DIR)):
            path = os.path.join(MODELS_DIR, name)
            # SavedModel directory
            if os.path.isdir(path) and os.path.isfile(os.path.join(path, "saved_model.pb")):
                models[name] = path
            # Keras single-file models (.keras, .h5)
            if os.path.isfile(path) and path.lower().endswith((".keras", ".h5", ".hdf5")):
                # use filename without extension as model name
                key = os.path.splitext(name)[0]
                models[key] = path
    return models

@st.cache_data(show_spinner=False)
def load_class_names(model_path: str):
    # 1) Buscar archivo con clases al lado del modelo
    candidates = [
        os.path.join(model_path, "classes.txt"),
        os.path.join(model_path, "labels.txt"),
        os.path.join(model_path, "class_indices.json"),
    ]
    # Intentar cargar desde los archivos
    for path in candidates:
        if os.path.isfile(path):
            try:
                if path.endswith(".json"):
                    import json
                    with open(path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    # data puede ser {class_name: index} o [class_name,...]
                    if isinstance(data, dict):
                        # Ordenar por √≠ndice
                        return [k for k, _ in sorted(data.items(), key=lambda kv: kv[1])]
                    elif isinstance(data, list):
                        return data
                else:
                    with open(path, "r", encoding="utf-8") as f:
                        lines = [ln.strip() for ln in f if ln.strip()]
                    if lines:
                        return lines
            except Exception:
                pass
    # 2) Fallback: usar clases del dataset de validaci√≥n , si existe    
    # S√≥lo intentar crear un dataset si la funci√≥n est√° disponible (TF presente)
    if image_dataset_from_directory is not None:
        try:
            _tmp_ds = image_dataset_from_directory(
                os.path.join("datos", "valid"), image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False
            )
            return list(_tmp_ds.class_names)
        except Exception:
            pass
    # Si no se puede inferir desde dataset, devolver lista vac√≠a para no romper la UI
    return []

AVAILABLE_MODELS = list_available_models()
if not AVAILABLE_MODELS:
    st.warning("No se encontraron modelos en la carpeta 'modelos/'.")
    st.stop()

# Descripciones breves por especie (editable) este es un arreglo
DESCRIPCIONES = {
    "Ara ararauna": "Guacamaya azul y amarilla; habita selvas tropicales, gran tama√±o y vocalizaciones fuertes.",
    "Chroicocephalus ridibundus": "Gaviota reidora; frecuente en humedales y costas, plumaje blanco con capucha estacional.",
    "Eubucco bourcierii": "Barbudito andino; peque√±o frug√≠voro de bosques montanos con colores vivos.",
    "Laterallus albigularis": "Polluela; ave de humedales y vegetaci√≥n densa, de h√°bitos esquivos.",
    "Melanerpes formicivorus": "Carpintero bellotero; social, almacena bellotas, t√≠pico de bosques abiertos.",
    "Patagioenas subvinacea": "Paloma subvin√°cea; habita bosques h√∫medos, de vuelo r√°pido y llamado profundo.",
    "Platalea ajaja": "Esp√°tula rosada; zancuda de color rosado con pico espatulado, forrajea en aguas someras.",
    "Rynchops niger": "Rayador americano; recorta la superficie del agua con su pico para capturar peces.",
    "Theristicus caudatus": "Ibis de cuello blanco; com√∫n en pastizales y humedales, pico curvo para sondar el suelo.",
    "Vultur gryphus": "C√≥ndor andino; gran carro√±ero de los Andes, una de las aves voladoras m√°s grandes.",
}

# Nombres comunes por especie (seg√∫n mapeo proporcionado)
COMMON_NAMES = {
    "Ara ararauna": "Guacamayo Azuliamarillo",
    "Chroicocephalus ridibundus": "Gaviota Reidora",
    "Eubucco bourcierii": "Cabez√≥n Cabecirrojo",
    "Laterallus albigularis": "Polluela Carrasqueadora",
    "Melanerpes formicivorus": "Carpintero Bellotero",
    "Patagioenas subvinacea": "Paloma Vinosa",
    "Platalea ajaja": "Esp√°tula Rosada",
    "Rynchops niger": "Rayador Americano",
    "Theristicus caudatus": "Bandurria Com√∫n",
    "Vultur gryphus": "C√≥ndor Andino",
}

# Funcion para preprocesar imagen seg√∫n m√©todo
def preprocess_image(file_like, method: str, img_size=None) -> np.ndarray:
    if img_size is None:
        img_size = IMG_SIZE
    img = load_img(file_like, target_size=img_size)
    arr = img_to_array(img)
    if method == "VGG16":
        x = np.expand_dims(arr, axis=0)
        x = vgg16_preprocess(x)
    else:
        x = np.expand_dims(arr / 255.0, axis=0)
    return x
# Preprocesar dataset seg√∫n m√©todo
def preprocess_dataset(ds, method: str, img_size=None):
    if img_size is None:
        img_size = IMG_SIZE
    if method == "VGG16":
        def _map(x, y):
            x2 = tf.numpy_function(lambda z: vgg16_preprocess(z), [x], tf.float32)
            x2.set_shape((None, img_size[0], img_size[1], 3))
            return x2, y
        return ds.map(_map)
    else:
        return ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))


def infer_model_image_size(model):
    """Intentar inferir (H, W) desde un modelo/objeto invocable.
    - Para tf.keras.Model, usa input_shape o inputs.
    - Para TFSMLayer o capas personalizadas, intenta buscar input_spec.
    Si no puede inferir, devuelve el valor por defecto IMG_SIZE.
    """
    try:
        shape = None
        # Keras Model
        if hasattr(model, "input_shape") and model.input_shape is not None:
            shape = model.input_shape
        elif hasattr(model, "inputs") and getattr(model, "inputs"):
            shape = model.inputs[0].shape
        # TFSMLayer y otros: intentar input_spec
        elif hasattr(model, "input_spec"):
            spec = getattr(model, "input_spec")
            if isinstance(spec, (list, tuple)):
                spec = spec[0]
            if hasattr(spec, "shape"):
                shape = spec.shape

        if shape is None:
            return IMG_SIZE

        # Normalizar a tupla de ints/None
        if hasattr(shape, "as_list"):
            shape_tuple = tuple(None if x is None else int(x) for x in shape.as_list())
        else:
            shape_tuple = tuple(shape)

        # Buscar H,W en la tupla. Soporta formatos (None,H,W,C) o (H,W,C) o (...,H,W,C)
        if len(shape_tuple) >= 3:
            # preferir los √∫ltimos 3 elementos
            H = shape_tuple[-3]
            W = shape_tuple[-2]
            if isinstance(H, int) and isinstance(W, int):
                return (H, W)
    except Exception:
        pass
    return IMG_SIZE
# Predecir probabilidades y asegurar formato
def predict_proba(x: np.ndarray) -> np.ndarray:
    preds = model(x)
    if isinstance(preds, dict):
        preds = list(preds.values())[0]
    if tf.is_tensor(preds):
        preds = preds.numpy()
    # Asegurar probabilidades (softmax) si no est√° normalizado
    if preds.ndim == 2:
        row_sums = preds.sum(axis=1, keepdims=True)
        if not np.all(np.isfinite(row_sums)) or np.any((row_sums < 0.99) | (row_sums > 1.01)):
            preds = tf.nn.softmax(preds, axis=1).numpy()
    return preds
# Buscar imagen de referencia para una clase
def find_reference_image(class_name: str):
    for split in ["test", "train"]:
        candidate_dir = os.path.join("datos", split, class_name)
        if os.path.isdir(candidate_dir):
            for fname in os.listdir(candidate_dir):
                if fname.lower().endswith((".jpg", ".jpeg", ".png")):
                    return os.path.join(candidate_dir, fname)
    return None

# ============================
# Utilidades de evaluaci√≥n
# ============================
def eval_on_dataset(model_layer, ds):
    Y_true, Y_pred = [], []
    for x_batch, y_batch in ds:
        pred = model_layer(x_batch)
        if isinstance(pred, dict):
            pred = list(pred.values())[0]
        if tf.is_tensor(pred):
            pred = pred.numpy()
        # Softmax si es necesario
        if pred.ndim == 2:
            row_sums = pred.sum(axis=1, keepdims=True)
            if not np.all(np.isfinite(row_sums)) or np.any((row_sums < 0.99) | (row_sums > 1.01)):
                pred = tf.nn.softmax(pred, axis=1).numpy()
        Y_pred.append(pred)
        Y_true.append(y_batch.numpy())
    Y_true = np.concatenate(Y_true)
    Y_pred = np.concatenate(Y_pred)
    y_true = Y_true.astype(int)
    y_pred = np.argmax(Y_pred, axis=1)
    # Top-3 accuracy
    top3 = np.mean([
        y_true[i] in np.argsort(Y_pred[i])[-3:]
        for i in range(len(y_true))
    ])
    acc = np.mean(y_true == y_pred)
    return acc, top3, y_true, y_pred, Y_pred

# Cachear evaluaci√≥n completa por (modelo, split, preprocesamiento)
@st.cache_data(show_spinner=False)
def evaluate_model_path(model_path: str, split: str, preprocess: str):
    mlayer = load_model_generic(model_path)
    # Infer model input size and use it when building the dataset / preprocessing
    try:
        inferred_size = infer_model_image_size(mlayer)
    except Exception:
        inferred_size = IMG_SIZE
    ds = get_dataset(split, img_size=inferred_size)
    ds = preprocess_dataset(ds, preprocess, img_size=inferred_size)
    acc, top3, y_true, y_pred, Y_pred = eval_on_dataset(mlayer, ds)
    labels = load_class_names(model_path)
    cm = confusion_matrix(y_true, y_pred)
    report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)
    return {
        "acc": acc,
        "top3": top3,
        "y_true": y_true,
        "y_pred": y_pred,
        "Y_pred": Y_pred,
        "labels": labels,
        "cm": cm,
        "report": report,
    }

# ============================
# Dashboard: barra lateral + secciones
# ============================
st.sidebar.title("‚öôÔ∏è Panel de control")
nav = st.sidebar.radio("Navegaci√≥n", ["üîç Predicci√≥n", "üìä Evaluaci√≥n"], index=0)

# Configuraci√≥n com√∫n
st.sidebar.markdown("---")
st.sidebar.subheader("Acerca del modelo")
# Selector de modelo
model_name = st.sidebar.selectbox("Modelo", options=list(AVAILABLE_MODELS.keys()), index=0)
MODEL_PATH = AVAILABLE_MODELS[model_name]
model = None
classes = []
classes_source = "desconocida"
if HAS_TF:
    try:
        model = load_model_generic(MODEL_PATH)
    except Exception as e:
        st.sidebar.error(f"No se pudo cargar el modelo: {e}")
        model = None
    try:
        classes = load_class_names(MODEL_PATH)
    except Exception:
        classes = []
else:
    # Sin TensorFlow no podemos cargar el modelo; intentar leer clases desde archivo si existe
    try:
        # Intentar cargar clases desde archivos al lado del modelo (sin TF)
        candidates = [
            os.path.join(MODEL_PATH, "classes.txt"),
            os.path.join(MODEL_PATH, "labels.txt"),
            os.path.join(MODEL_PATH, "class_indices.json"),
        ]
        for path in candidates:
            if os.path.isfile(path):
                if path.endswith('.json'):
                    import json
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, dict):
                        classes = [k for k, _ in sorted(data.items(), key=lambda kv: kv[1])]
                    elif isinstance(data, list):
                        classes = data
                    break
                else:
                    with open(path, 'r', encoding='utf-8') as f:
                        lines = [ln.strip() for ln in f if ln.strip()]
                    if lines:
                        classes = lines
                        break
    except Exception:
        classes = []
classes_source = "archivo del modelo" if any(
    os.path.isfile(os.path.join(MODEL_PATH, name)) for name in ["classes.txt", "labels.txt", "class_indices.json"]
) else ("dataset" if classes else "desconocida")

st.sidebar.write(f"Seleccionado: `{model_name}`")
st.sidebar.write(f"Ruta: `{MODEL_PATH}`")
st.sidebar.write(f"Clases: {len(classes)}")
st.sidebar.caption(f"Fuente de clases: {classes_source}")

# Si no hay TensorFlow/PennyLane en este entorno, mostrar aviso claro
if not HAS_TF or not HAS_PENNYLANE:
    msg_lines = []
    if not HAS_TF:
        msg_lines.append("TensorFlow no est√° instalado en este despliegue; la evaluaci√≥n y carga de modelos .keras/.h5 estar√° deshabilitada.")
    if not HAS_PENNYLANE:
        msg_lines.append("PennyLane no est√° instalado; los modelos h√≠bridos cu√°nticos no se podr√°n ejecutar aqu√≠.")
    st.sidebar.warning("\n".join(msg_lines))

# Diagn√≥stico opcional de la arquitectura y capas
if st.sidebar.checkbox("Mostrar diagn√≥stico de modelo", value=False):
    try:
        diag = get_model_diagnostics(model)
        with st.expander("Diagn√≥stico del modelo (capas)"):
            for d in diag:
                st.write("‚Äî" * 40)
                st.write(f"Nombre: {d.get('name')}")
                st.write(f"Tipo: {d.get('class')}")
                st.write(f"input_shape: {d.get('input_shape')}")
                st.write(f"output_shape: {d.get('output_shape')}")
                if d.get("extra"):
                    st.write("Extra:")
                    st.json(d.get("extra"))
    except Exception as e:
        st.warning(f"No se pudo generar el diagn√≥stico: {e}")

if nav.startswith("üîç"):
    # Controles de predicci√≥n (versi√≥n √∫nica y ordenada)
    st.sidebar.markdown("---")
    st.sidebar.subheader("Opciones de predicci√≥n")
    prep_method = st.sidebar.selectbox("Preprocesamiento", ["x/255", "VGG16"], index=0)
    num_classes = max(1, len(classes))
    top_k = st.sidebar.slider("Top-K", min_value=1, max_value=min(10, num_classes), value=min(5, num_classes))
    show_ref = st.sidebar.checkbox("Mostrar imagen de referencia", value=True)

    st.subheader("Sube una imagen de un ave")
    up = st.file_uploader("Elige una imagen (JPG/PNG)", type=["jpg", "jpeg", "png"])

    # Variables por defecto
    probs = np.array([])
    sel_class = None
    sel_common = None
    sel_conf = 0.0

    if up is not None:
        if not HAS_TF or load_img is None:
            st.error("TensorFlow/Keras no est√°n disponibles en este despliegue; no se pueden realizar predicciones aqu√≠. Cargue el modelo localmente o despliegue en un runtime compatible.")
        else:
            c1, c2 = st.columns([1, 1])
            with c1:
                st.image(up, caption="Imagen cargada", use_container_width=True)
            # Preprocesar y predecir
            try:
                x = preprocess_image(up, prep_method)
                probs_all = predict_proba(x)
                if probs_all is None:
                    raise RuntimeError("predict_proba devolvi√≥ None")
                probs = np.asarray(probs_all)
                if probs.ndim == 2 and probs.shape[0] >= 1:
                    probs = probs[0]
            except Exception as e:
                st.error(f"Error durante la predicci√≥n: {e}")

    # Si no hay probabilidades v√°lidas, evitamos renderizar Top-K
    if probs.size == 0:
        st.info("Sube una imagen v√°lida para ver predicciones Top-K.")
    else:
        # Determinar lista de etiquetas (clases) de forma segura
        if classes and len(classes) > 0:
            cls_list = classes
        else:
            cls_list = load_class_names(MODEL_PATH) or []
        if not cls_list:
            # construir etiquetas gen√©ricas
            try:
                n_out = int(probs.shape[-1])
            except Exception:
                n_out = int(np.asarray(probs).size)
            cls_list = [f"class_{i}" for i in range(n_out)]

        # Top-K visual: im√°genes en columnas con bot√≥n debajo (√∫nico bloque)
        suggest_k = min(3, len(cls_list))
        top_idx_sorted = np.argsort(probs)[-suggest_k:][::-1] if suggest_k > 0 else np.array([], dtype=int)
        default_idx = int(top_idx_sorted[0]) if top_idx_sorted.size > 0 else 0
        default_class = cls_list[default_idx] if default_idx < len(cls_list) else f"class_{default_idx}"
        default_conf = float(probs[default_idx]) if default_idx < probs.size else 0.0

        # Preparar opciones y mapeos
        options = []
        idx_map = {}
        for idx in top_idx_sorted:
            sci = cls_list[int(idx)]
            com = COMMON_NAMES.get(sci, sci)
            label = f"{com} ({sci}) ‚Äî {probs[int(idx)]*100:.1f}%"
            options.append(label)
            idx_map[label] = int(idx)

        # Mostrar m√©tricas + im√°genes en columnas
        with c2:
            st.metric("Predicci√≥n sugerida", COMMON_NAMES.get(default_class, default_class))
            st.metric("Confianza", f"{default_conf*100:.2f}%")
            st.markdown("### Opciones Top-3")
            cols = st.columns(max(1, suggest_k))
            sel_key = f"sel_idx_pred_{model_name}"
            selected_idx = st.session_state.get(sel_key, default_idx)
            # Renderizar cada opci√≥n en su columna con bot√≥n debajo
            for j, idx in enumerate(top_idx_sorted):
                idx = int(idx)
                sci = cls_list[idx]
                com = COMMON_NAMES.get(sci, sci)
                ref = find_reference_image(sci)
                with cols[j]:
                    if ref:
                        st.image(ref, caption=f"{com} ‚Äî {probs[idx]*100:.1f}%", use_container_width=True)
                    else:
                        st.caption(f"{com} ‚Äî {probs[idx]*100:.1f}% (sin imagen de referencia)")
                    # Bot√≥n √∫nico por columna
                    if st.button("Elegir", key=f"btn_choose_{model_name}_{j}_{idx}"):
                        st.session_state[sel_key] = int(idx)
                        selected_idx = int(idx)

            # Radio para elegir entre las Top-K (√∫til si se prefiere lista)
            try:
                default_radio_index = list(top_idx_sorted).index(int(selected_idx))
            except Exception:
                default_radio_index = 0
            choice = st.radio("¬øCu√°l encaja mejor?", options=options, index=default_radio_index)
            sel_idx = idx_map.get(choice, int(top_idx_sorted[0]) if top_idx_sorted.size > 0 else 0)
            if sel_idx != selected_idx:
                st.session_state[sel_key] = int(sel_idx)
            sel_class = cls_list[int(sel_idx)]
            sel_common = COMMON_NAMES.get(sel_class, sel_class)
            sel_conf = float(probs[int(sel_idx)])

            # Mostrar detalles para la selecci√≥n
            st.write(f"Nombre cient√≠fico: **{sel_class}**")
            st.write(f"Nombre com√∫n: **{sel_common}**")
            desc = DESCRIPCIONES.get(sel_class, "Descripci√≥n no disponible.")
            st.write(desc)

        # Imagen de referencia principal (a la derecha de la selecci√≥n)
        if show_ref and sel_class:
            ref_path = find_reference_image(sel_class)
            if ref_path:
                st.image(ref_path, caption=f"Ejemplo de {sel_class}", use_container_width=True)

        # Gr√°fico Top-K
        k = min(top_k, len(cls_list))
        top_k_idx = np.argsort(probs)[-k:][::-1]
        top_labels = [COMMON_NAMES.get(cls_list[int(i)], cls_list[int(i)]) for i in top_k_idx]
        top_df = pd.DataFrame({
            "Clase (com√∫n)": top_labels,
            "Probabilidad": [float(probs[int(i)]) for i in top_k_idx],
        })
        st.bar_chart(top_df.set_index("Clase (com√∫n)"))

else:
    # Controles de evaluaci√≥n en la barra lateral
    st.sidebar.markdown("---")
    st.sidebar.subheader("Opciones de evaluaci√≥n")
    split = st.sidebar.selectbox("Conjunto", options=["valid", "test"], index=0)
    prep_method_eval = st.sidebar.selectbox("Preprocesamiento", ["x/255", "VGG16"], index=0)
    eval_button = st.sidebar.button("Calcular m√©tricas", use_container_width=True)
    st.sidebar.subheader("Comparaci√≥n de modelos")
    compare_models = st.sidebar.multiselect("Modelos a comparar", options=list(AVAILABLE_MODELS.keys()), default=[model_name])
    compare_button = st.sidebar.button("Comparar modelos", use_container_width=True)
    # establecer secci√≥n de evaluaci√≥n
    st.subheader("Evaluaci√≥n del modelo")
    if eval_button:
        if not HAS_TF:
            st.error("TensorFlow no est√° disponible; la evaluaci√≥n est√° deshabilitada en este despliegue.")
        else:
            ds = get_dataset(split)
            ds = preprocess_dataset(ds, prep_method_eval)
        labels = classes
        Y_true, Y_pred = [], []
        progress = st.progress(0, text="Calculando predicciones...")
        # Intentar estimar n√∫mero total de lotes
        try:
            total_batches = int(tf.data.experimental.cardinality(ds).numpy())
        except Exception:
            total_batches = None
        for batch_idx, (x_batch, y_batch) in enumerate(ds):
            pred = predict_proba(x_batch)
            Y_pred.append(pred)
            Y_true.append(y_batch.numpy())
            if total_batches and total_batches > 0:
                progress.progress((batch_idx + 1) / total_batches)
        progress.progress(1.0)
        progress.empty()

        # Aplanar y calcular m√©tricas
        Y_true = np.concatenate(Y_true)  # Etiquetas enteras (shape: [N,])
        Y_pred = np.concatenate(Y_pred)  # Probabilidades (shape: [N, C])
        y_true = Y_true.astype(int)      # Etiquetas ya son √≠ndices de clase
        y_pred = np.argmax(Y_pred, axis=1)

        st.markdown("### M√©tricas generales")
        acc = np.mean(y_true == y_pred)
        c1, c2, c3 = st.columns(3)
        c1.metric("Accuracy", f"{acc*100:.2f}%")
        c2.metric("Im√°genes", str(len(y_true)))
        c3.metric("Clases", str(len(labels)))

        st.markdown("### Matriz de confusi√≥n")
        cm = confusion_matrix(y_true, y_pred)
        fig, ax = plt.subplots(figsize=(6, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels, ax=ax)
        ax.set_ylabel("Real")
        ax.set_xlabel("Predicci√≥n")
        st.pyplot(fig, use_container_width=True)

        st.markdown("### Reporte de clasificaci√≥n")
        report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)
        df_report = pd.DataFrame(report).transpose()
        st.dataframe(
            df_report.style.format({
                "precision": "{:.2f}",
                "recall": "{:.2f}",
                "f1-score": "{:.2f}",
                "support": "{:.0f}",
            }),
            use_container_width=True,
        )

        # Descargar CSV
        csv = df_report.to_csv().encode("utf-8")
        st.download_button(
            label=f"Descargar reporte ({split}).csv",
            data=csv,
            file_name=f"reporte_{split}.csv",
            mime="text/csv",
        )
    # ============================
    # Comparaci√≥n de modelos
    # ============================
    if compare_button and compare_models:
        st.markdown("## Comparaci√≥n de modelos")
        # Resumen r√°pido
        rows = []
        progress = st.progress(0, text="Evaluando modelos seleccionados...")
        n_models = len(compare_models)
        eval_results = {}
        for idx, mname in enumerate(compare_models):
            mpath = AVAILABLE_MODELS[mname]
            if not HAS_TF:
                st.error("TensorFlow no est√° disponible; no se puede evaluar modelos en este despliegue.")
                res = None
            else:
                res = evaluate_model_path(mpath, split, prep_method_eval)
            eval_results[mname] = res
            rows.append({
                "Modelo": mname,
                "Accuracy": res["acc"] if res is not None else None,
                "Top-3": res["top3"] if res is not None else None,
                "Im√°genes": len(res["y_true"]) if res is not None else 0,
            })
            progress.progress((idx + 1) / n_models)
        progress.empty()

        df_cmp = pd.DataFrame(rows)
        st.dataframe(df_cmp.style.format({"Accuracy": "{:.2%}", "Top-3": "{:.2%}"}), use_container_width=True)
        # Gr√°fico de barras de accuracy
        chart_df = df_cmp.set_index("Modelo")["Accuracy"]
        st.bar_chart(chart_df)

        # Detalle por modelo (igual que evaluaci√≥n individual)
        st.markdown("### Detalle por modelo")
        for mname in compare_models:
            res = eval_results[mname]
            if res is None:
                st.warning(f"No hay resultados para {mname} (TensorFlow no disponible en este despliegue).")
                continue
            labels_m = res["labels"]
            y_true_m = res["y_true"]
            y_pred_m = res["y_pred"]
            cm_m = res["cm"]
            report_m = res["report"]

            with st.expander(f"{mname}", expanded=False):
                c1, c2, c3 = st.columns(3)
                c1.metric("Accuracy", f"{res['acc']*100:.2f}%")
                c2.metric("Top-3", f"{res['top3']*100:.2f}%")
                c3.metric("Im√°genes", str(len(y_true_m)))

                st.markdown("**Matriz de confusi√≥n**")
                fig_m, ax_m = plt.subplots(figsize=(6, 6))
                sns.heatmap(cm_m, annot=True, fmt="d", cmap="Blues", xticklabels=labels_m, yticklabels=labels_m, ax=ax_m)
                ax_m.set_ylabel("Real")
                ax_m.set_xlabel("Predicci√≥n")
                st.pyplot(fig_m, use_container_width=True)

                st.markdown("**Reporte de clasificaci√≥n**")
                df_rep_m = pd.DataFrame(report_m).transpose()
                st.dataframe(
                    df_rep_m.style.format({
                        "precision": "{:.2f}",
                        "recall": "{:.2f}",
                        "f1-score": "{:.2f}",
                        "support": "{:.0f}",
                    }),
                    use_container_width=True,
                )

                csv_m = df_rep_m.to_csv().encode("utf-8")
                st.download_button(
                    label=f"Descargar reporte ({mname} ¬∑ {split}).csv",
                    data=csv_m,
                    file_name=f"reporte_{mname}_{split}.csv",
                    mime="text/csv",
                )
    else:
        st.info("Selecciona el conjunto en la barra lateral y pulsa 'Calcular m√©tricas'.")

# Estilos suaves
st.markdown(
    """
    <style>
    .stMetric { text-align: center; }
    </style>
    """,
    unsafe_allow_html=True,
)
